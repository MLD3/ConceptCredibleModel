{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expected-investigator",
   "metadata": {},
   "source": [
    "# Credible Concept Model (CCM) tutorial: Extend credible model to uninterpretable raw features\n",
    "\n",
    "By the end of the tutorial, one should be able to\n",
    "\n",
    "1. understand the motivation of CCM\n",
    "2. understand the CCM algorithm\n",
    "3. able to train an CCM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "constant-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-healing",
   "metadata": {},
   "source": [
    "# A motivating example\n",
    "\n",
    "Despite the fact that model interpretation suffers from faithfulness and fragility issues (and also people's overtrust of the interpretation), it is still useful to enforce prior knowledge. Feature attribution tools often assume that features are interpretable. This makes them less applicable when raw features are complicated (say in the image or audio domain). Thus, a lot of recent research effort have been poured in to explanation with high level concepts. Among them is this idea of concept bottleneck network (CBM). In this work, we expose the flaw of CBM and suggest a simple fix.\n",
    "\n",
    "This examples shows why missing concept is a nastier problem than one would think.\n",
    "\n",
    "$Z \\sim U(\\{0, 1\\})^d$ are $d$ latent concepts\n",
    "\n",
    "$X = project(Z)$ is a random linear projection of concepts presented to the human (the following just used identity for simplicity)\n",
    "\n",
    "$X \\rightarrow C \\rightarrow X$ where $C \\in \\mathbb{R}^{d-1}$ is concept detectors\n",
    "\n",
    "$Y = and(Z)$ where $\\theta$ is the true coefficient.\n",
    "\n",
    "This setup will fail raw feature attribution (because $X$ distribute concepts across dimensions), concept discovery (when the function is a \"and\" concept), jointly trained concept (because $C$ has 1 less dimension off $Z$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fossil-generation",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 10000\n",
    "d = 3\n",
    "bs = 100\n",
    "n_concepts = 1 #d-1\n",
    "\n",
    "def gen_data(n, d):\n",
    "    Z = np.random.choice([0, 1], (n, d))\n",
    "    X = Z\n",
    "    Y = np.logical_and(*[Z[:, i] for i in range(Z.shape[1])]) # and of the arguments\n",
    "\n",
    "    return torch.from_numpy(X).float(), torch.from_numpy(Y).long(), torch.from_numpy(Z).float()\n",
    "\n",
    "X, Y, Z = gen_data(n, d)\n",
    "X_te, Y_te, Z_te = gen_data(n, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unauthorized-corpus",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, neuron_sizes, activation=nn.ReLU, bias=True): \n",
    "        super(MLP, self).__init__()\n",
    "        self.neuron_sizes = neuron_sizes\n",
    "        \n",
    "        layers = []\n",
    "        for s0, s1 in zip(neuron_sizes[:-1], neuron_sizes[1:]):\n",
    "            layers.extend([\n",
    "                nn.Linear(s0, s1, bias=bias),\n",
    "                activation()\n",
    "            ])\n",
    "        \n",
    "        self.classifier = nn.Sequential(*layers[:-1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.neuron_sizes[0])\n",
    "        return self.classifier(x)\n",
    "    \n",
    "def train_step_standard(net, loader, opt, criterion, device='cpu'):\n",
    "    losses = []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        o = net(x)\n",
    "        l = criterion(o, y).mean()\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "        losses.append(l.detach().item())\n",
    "    return losses\n",
    "    \n",
    "def train(net, loader, opt, train_step=train_step_standard, \n",
    "          criterion=F.cross_entropy, n_epochs=10, report_every=1, device=\"cpu\"):\n",
    "    net.train()\n",
    "    train_log, losses = [], []\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        _losses = train_step(net, loader, opt, criterion, device=device)\n",
    "        losses.extend(_losses)\n",
    "        \n",
    "        # for reporting\n",
    "        if i % report_every == 0:\n",
    "            \n",
    "            train_report = {\"loss\": np.mean(losses[-len(loader):])}\n",
    "            \n",
    "            # print out report\n",
    "            print('epoch {:>3}: '.format(i) + ' '.join('{} {:.3e}'.format(\n",
    "                      name, val\n",
    "                  ) for name, val in train_report.items()))\n",
    "\n",
    "            train_report.update({'epoch': i})\n",
    "            train_log.append(train_report)\n",
    "\n",
    "    return train_log    \n",
    "    \n",
    "def test(net, loader, criterion):\n",
    "    net.eval()\n",
    "    losses = []\n",
    "    total = 0\n",
    "    for x, y in loader:\n",
    "        o = net(x)\n",
    "        l = criterion(o, y).mean()\n",
    "        bs = o.shape[0]\n",
    "        total += bs        \n",
    "        losses.append(l.detach().item() * bs)\n",
    "    net.train()\n",
    "    return sum(losses) / total\n",
    "\n",
    "# report accuracy\n",
    "acc_criterion = lambda o, y: (o.argmax(1) == y).float()\n",
    "\n",
    "# dataset\n",
    "loader_xy = DataLoader(TensorDataset(X, Y), batch_size=bs, shuffle=True) # regular dataset\n",
    "loader_xyz = DataLoader(TensorDataset(X, Y, Z), batch_size=bs, shuffle=True) # joint concept and regular dataset\n",
    "\n",
    "loader_xy_te = DataLoader(TensorDataset(X_te, Y_te), batch_size=bs, shuffle=True) # regular dataset\n",
    "loader_xyz_te = DataLoader(TensorDataset(X_te, Y_te, Z_te), batch_size=bs, shuffle=True) # joint concept and regular dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-livestock",
   "metadata": {},
   "source": [
    "## standard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "grave-headset",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaxuan/.local/share/virtualenvs/CredibleConceptModel-7quSdsQm/lib/python3.7/site-packages/torch/autograd/__init__.py:147: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0: loss 5.377e-01\n",
      "epoch   1: loss 3.986e-01\n",
      "epoch   2: loss 2.661e-01\n",
      "epoch   3: loss 1.693e-01\n",
      "epoch   4: loss 1.081e-01\n",
      "epoch   5: loss 7.109e-02\n",
      "epoch   6: loss 4.883e-02\n",
      "epoch   7: loss 3.499e-02\n",
      "epoch   8: loss 2.604e-02\n",
      "epoch   9: loss 2.001e-02\n",
      "task acc after training: 100.0%\n"
     ]
    }
   ],
   "source": [
    "def standard_model(loader_xy, d):\n",
    "    # regular model\n",
    "    net = MLP([d, d * 2, 2])\n",
    "\n",
    "    # train\n",
    "    opt = optim.Adam(net.parameters())\n",
    "    train(net, loader_xy, opt)\n",
    "    return net\n",
    "\n",
    "standard_net = standard_model(loader_xy, d)\n",
    "print('task acc after training: {:.1f}%'.format(test(standard_net, loader_xy_te, acc_criterion) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-cinema",
   "metadata": {},
   "source": [
    "## joint EBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sharp-environment",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of concepts to learn: 1\n",
      "true number of concepts: 3\n",
      "epoch   0: loss 1.251e-02\n",
      "epoch   1: loss 1.123e-02\n",
      "epoch   2: loss 9.502e-03\n",
      "epoch   3: loss 7.712e-03\n",
      "epoch   4: loss 6.223e-03\n",
      "epoch   5: loss 4.381e-03\n",
      "epoch   6: loss 3.022e-03\n",
      "epoch   7: loss 2.431e-03\n",
      "epoch   8: loss 2.056e-03\n",
      "epoch   9: loss 1.759e-03\n"
     ]
    }
   ],
   "source": [
    "class EBM(nn.Module):\n",
    "    '''\n",
    "    net_c is concept net, net_y is the task net\n",
    "    it output net_y(net_c(x))\n",
    "    '''\n",
    "\n",
    "    def __init__(self, net_c, net_y): \n",
    "        super(EBM, self).__init__()\n",
    "        self.net_c = net_c\n",
    "        self.net_y = net_y\n",
    "        \n",
    "        self.classifier = nn.Sequential(net_c, net_y)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "def train_step_xyz(net, loader, opt, criterion, device='cpu'):\n",
    "    '''training step for EBM jont model'''\n",
    "    assert type(net) == EBM, f\"must use EBM model; currently {type(net)}\"\n",
    "    losses = []\n",
    "    for x, y, z in loader:\n",
    "        x, y, z = x.to(device), y.to(device), z.to(device)\n",
    "        opt.zero_grad()\n",
    "        o_z = net.net_c(x)\n",
    "        o_y = net.net_y(o_z)\n",
    "        l = criterion(o_y, y, o_z, z).mean()\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "        losses.append(l.detach().item())\n",
    "    return losses\n",
    "\n",
    "def joint_EBM(d, n_concepts, loader_xyz):\n",
    "    n_concepts = min(d, n_concepts)\n",
    "    print(f'number of concepts to learn: {n_concepts}')\n",
    "    print(f'true number of concepts: {d}')\n",
    "    \n",
    "    # EBM with concept and task jointly trained\n",
    "    net_c = MLP([d, d * 2, n_concepts])\n",
    "    net_y = MLP([n_concepts, d * 2, 2])\n",
    "    net = EBM(net_c, net_y)\n",
    "    \n",
    "    # train\n",
    "    def criterion(o_y, y, o_z, z):\n",
    "        y_loss = F.cross_entropy(o_y, y).sum()\n",
    "        z_loss = F.binary_cross_entropy(torch.sigmoid(o_z), z[:, :n_concepts]).sum()\n",
    "        return (y_loss + z_loss) / len(y)\n",
    "    \n",
    "    opt = optim.Adam(net.parameters())\n",
    "    train(net, loader_xyz, opt, train_step=train_step_xyz, criterion=criterion)\n",
    "\n",
    "    return net\n",
    "\n",
    "ebm_joint = joint_EBM(d, n_concepts, loader_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "continued-comment",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task acc after training: 100.0%\n",
      "concept 0 acc after training: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# tobe cleaned: now assumes net_c output binary prediction\n",
    "def test_concept(net, concept_idx, loader):\n",
    "    '''net output many logit for concepts, concept_idx is the concept index'''\n",
    "    net.eval()\n",
    "    losses = []\n",
    "    total = 0\n",
    "    for x, y in loader:\n",
    "        o = net(x) # (bs, n_concept)\n",
    "        o2 = torch.sigmoid(o[:, concept_idx]) > 0.5\n",
    "        l = (o2 == y).float().mean()\n",
    "        bs = o.shape[0]\n",
    "        total += bs        \n",
    "        losses.append(l.detach().item() * bs)\n",
    "    net.train()\n",
    "    return sum(losses) / total\n",
    "\n",
    "def print_concept_acc(net_c, n_concepts, X_te, Z_te):\n",
    "    for i in range(n_concepts):\n",
    "        print('concept {} acc after training: {:.1f}%'.format(i, test_concept(net_c,\n",
    "                                                                              i,\n",
    "                                                                              DataLoader(TensorDataset(X_te, Z_te[:, i]), batch_size=bs)) * 100))\n",
    "\n",
    "print('task acc after training: {:.1f}%'.format(test(ebm_joint, loader_xy_te, acc_criterion ) * 100))\n",
    "print_concept_acc(ebm_joint.net_c, n_concepts, X_te, Z_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-characteristic",
   "metadata": {},
   "source": [
    "## independent EBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "voluntary-nature",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of concepts to learn: 1\n",
      "true number of concepts: 3\n",
      "training concepts ...\n",
      "epoch   0: loss 6.535e-01\n",
      "epoch   1: loss 5.960e-01\n",
      "epoch   2: loss 5.307e-01\n",
      "epoch   3: loss 4.431e-01\n",
      "epoch   4: loss 3.521e-01\n",
      "epoch   5: loss 2.650e-01\n",
      "epoch   6: loss 1.850e-01\n",
      "epoch   7: loss 1.317e-01\n",
      "epoch   8: loss 9.734e-02\n",
      "epoch   9: loss 7.424e-02\n",
      "training tasks ...\n",
      "epoch   0: loss 6.529e-01\n",
      "epoch   1: loss 5.578e-01\n",
      "epoch   2: loss 4.846e-01\n",
      "epoch   3: loss 4.315e-01\n",
      "epoch   4: loss 3.997e-01\n",
      "epoch   5: loss 3.815e-01\n",
      "epoch   6: loss 3.706e-01\n",
      "epoch   7: loss 3.638e-01\n",
      "epoch   8: loss 3.596e-01\n",
      "epoch   9: loss 3.562e-01\n"
     ]
    }
   ],
   "source": [
    "def ind_EBM(d, n_concepts, X, Y, Z):\n",
    "    n_concepts = min(d, n_concepts)\n",
    "    print(f'number of concepts to learn: {n_concepts}')\n",
    "    print(f'true number of concepts: {d}')\n",
    "    \n",
    "    # EBM with concept and task independently trained\n",
    "    net_c = nn.Sequential(MLP([d, d * 2, n_concepts]), nn.Sigmoid())\n",
    "    net_y = MLP([n_concepts, d * 2, 2])\n",
    "    net = EBM(net_c, net_y)\n",
    "    \n",
    "    # train\n",
    "    print('training concepts ...')\n",
    "    opt_c = optim.Adam(net_c.parameters())\n",
    "    train(net_c, \n",
    "          DataLoader(TensorDataset(X, Z[:, :n_concepts]), batch_size=bs, shuffle=True), \n",
    "          opt_c,\n",
    "          criterion=nn.BCELoss())\n",
    "\n",
    "    print('training tasks ...')\n",
    "    opt_y = optim.Adam(net_y.parameters())\n",
    "    train(net_y, \n",
    "          DataLoader(TensorDataset(Z[:, :n_concepts], Y), batch_size=bs, shuffle=True), \n",
    "          opt_y)\n",
    "\n",
    "    return net\n",
    "\n",
    "ebm_ind = ind_EBM(d, n_concepts, X, Y, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cathedral-baseline",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task acc after training: 74.7%\n",
      "concept 0 acc after training: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print('task acc after training: {:.1f}%'.format(test(ebm_ind, loader_xy_te, acc_criterion ) * 100)) # the optimum is 1/16 + 3/4; the strategy is to randomize if the first concept is 1, otherwise output 0\n",
    "print_concept_acc(ebm_ind.net_c[0], n_concepts, X_te, Z_te) # net_c[0] because it is the logit part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-person",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
